options(scipen=999)
load("~/Dropbox/Mugshots Project/Code/R Analysis/NIBRS Offenders 2013/models.RData")
subsets <- list(simple_assault_data, aggravated_assault_data, intimidation_data, robbery_data,
shoplifting_data, vandalism_data, drugs_narcotics_data, drug_equipment_data)
robbery_data$unemp_onerace
robbery_data$unem
names(robbery_data)
robbery_data$mean_inc_standardized
robbery_data$one_race_unemp_standardized
load("~/Dropbox/Mugshots Project/Code/R Analysis/NIBRS Offenders 2013/basic.RData")
# Or, read the merged and cleaned data and clean it further
setwd("~/Dropbox/Mugshots Project/Public/Data/")
dataset = read.csv("offenders_lemas_acs_2013_clean.csv")
# Only include local police
dataset <- subset(dataset, dataset$type == 1)
# Undo mistake of multiplying ratio by 100
dataset$wofficers_divres = dataset$wofficers_divres / 100
# To only examine men
# dataset <- subset(dataset, dataset$sex == 'male')
# To only include offenders with a white victim
# dataset <- subset(dataset, dataset$victim_analyzing_race == 'white')
setwd("~/Dropbox/Mugshots Project/Code/R Analysis/NIBRS Offenders 2013")
source("remove_na.R")
source("standardize_vars.R")
source("subsets.R")
source("remove_sparse.R")
library(rms)
library(robustbase)
subsets <- list(simple_assault_data, aggravated_assault_data, intimidation_data, robbery_data,
shoplifting_data, vandalism_data, drugs_narcotics_data, drug_equipment_data)
clogit_models <- lapply(subsets, function(subset) {
model <- lrm(arrested ~ sex + black_not_white * w_officers_percent_standardized +
male_officers_percent_standardized + ftsworn_standardized +
com_bt + tech_typ_vpat + hir_trn_no_p + min_hiring_educ_gths +
bdgt_ttl_standardized + ftsworn_standardized +
w_residents_percent_standardized + mean_inc_standardized +
one_race_unemp_standardized,
x = T, y = T, data = subset)
results <- robcov(model, cluster = subset$ori)
results
})
clogit_models
clogit_models_all_controls <- clogit_models
rm(clogit_models)
clogit_models_police_controls <- lapply(subsets, function(subset) {
model <- lrm(arrested ~ sex + black_not_white * w_officers_percent_standardized +
male_officers_percent_standardized + ftsworn_standardized +
com_bt + tech_typ_vpat + hir_trn_no_p + min_hiring_educ_gths +
bdgt_ttl_standardized + ftsworn_standardized,
x = T, y = T, data = subset)
results <- robcov(model, cluster = subset$ori)
results
})
clogit_models_police_controls[1]
clogit_models_police_controls[2]
clogit_models_police_controls[3]
clogit_models_police_controls[4]
clogit_models_police_controls[5]
clogit_models_police_controls[6]
clogit_models_police_controls[7]
clogit_models_police_controls[8]
clogit_models <- lapply(subsets, function(subset) {
model <- lrm(arrested ~ sex + black_not_white * w_officers_percent_standardized +
male_officers_percent_standardized + ftsworn_standardized +
com_bt + tech_typ_vpat + hir_trn_no_p + min_hiring_educ_gths,
x = T, y = T, data = subset)
results <- robcov(model, cluster = subset$ori)
results
})
help(glmrob)
model0_logit <- glmrob(arrested ~ black_not_white * w_officers_percent_standardized,
data = simple_assault_data, family = binomial(link = "logit"),
x = T, y = T)
summary(model0_logit)
model0_clustered_logit <- lrm(arrested ~ black_not_white * w_officers_percent_standardized,
x = T, y = T, data = simple_assault_data)
model0_clustered_logit_result <- robcov(model0_clustered_logit, cluster = subset$ori)
model0_clustered_logit_result <- robcov(model0_clustered_logit, cluster = simple_assault_data$ori)
model0_clustered_logit_result
model0_clustered_logit <- lrm(arrested ~ black_not_white * w_officers_percent_standardized,
x = T, y = T, data = shoplifting_data)
model0_clustered_logit_result <- robcov(model0_clustered_logit, cluster = shoplifting_data$ori)
model0_clustered_logit_result
model0_clustered_logit <- lrm(arrested ~ black_not_white * w_officers_percent_standardized,
x = T, y = T, data = drugs_narcotics_data)
model0_clustered_logit_result <- robcov(model0_clustered_logit, cluster = drugs_narcotics_data$ori)
model0_clustered_logit_result
save.image("~/Dropbox/Mugshots Project/Code/R Analysis/NIBRS Offenders 2013/basic.RData")
library(ISLR)
library(class)
help(class)
Caravan
dataset <- Caravan
names(dataset)
mean(dataset$Purchase)
mean(dataset$Purchase, rm.na = T)
mean(dataset$Purchase, na.rm = T)
dataset$Purchase
table(dataset$Purchase)
prop.table(table(dataset$Purchase))
help(scale)
dataset[dataset$Purchase == 'No']$purchase_binary <- 0
dataset$purchase_binary[dataset$Purchase == 'No'] <- 0
dataset$purchase_binary[dataset$Purchase == 'Yes'] <- 1
table()
table(dataset$purchase_binary)
scale(dataset$purchase_binary)
dataset$purchase_binary <- scale(dataset$purchase_binary)
table(dataset$purchase_binary)
mean(dataset$purchase_binary, na.rm = T)
sd(dataset$purchase_binary, na.rm = T)
dataset <- Caravan
prop.table(table(dataset$Purchase))
dataset$purchase_binary[dataset$Purchase == 'No'] <- 0
dataset$purchase_binary[dataset$Purchase == 'Yes'] <- 1
dataset$purchase_binary_rescale <- scale(dataset$purchase_binary)
help(knn)
random_1000 <- sample(1:nrow(dataset), 1000, replace = FALSE)
random_1000[0:10]
training_data <- dataset[random_1000,]
test_data <- dataset[random_1000:,]
test_data <- dataset[,random_1000:]
test_data <- dataset[,random_1000]
test_data <- dataset[!random_1000]
test_data <- dataset[!(i %in% random_1000)]
range(1:nrow(dataset)) %in% random_1000
range(1:nrow(dataset))
range(nrow(dataset)
)
list(1:10)
total_index <- list(1:nrow(dataset))
total_index[!(total_index %in% random_1000)]
test_index <- total_index[!(total_index %in% random_1000)]
test_data <- dataset[test_index,]
training_data <- dataset[random_1000,]
test_index
sample(test_index)
test_data <- dataset[sample(test_index),]
test_data <- dataset[[test_index],]
total_index <- 1:nrow(dataset)
test_index <- total_index[!(total_index %in% random_1000)]
test_data <- dataset[test_index,]
test_data <- dataset[-random_1000,]
train.Y = test_data$purchase_binary_rescale[-test_index]
test.Y = test_data$purchase_binary_rescale[test_index]
train.X = test_data[,-86]$purchase_binary_rescale[-test_index]
test.X = test_data[,-86]$purchase_binary_rescale[test_index]
standardized.X <- scale(dataset[,-86])
standardized.X <- scale(dataset[,-86])
train.Y = test_data$purchase_binary_rescale[-test_index]
test.Y = test_data$purchase_binary_rescale[test_index]
train.X = standardized.X[-test_index]
test.X = standardized.X[test_index]
knn(training_data, test_data, train.Y)
knn(train.X, test_data, train.Y, 1)
knn(train.X, test.X, train.Y, 1)
knn(train.X, test.X, [train.Y, 1], 1)
train.Y = c(test_data$purchase_binary_rescale[-test_index])
knn(train.X, test.X, train.Y, 1)
standardized.X <- scale(dataset[,-86])
# Random 1000 index for test data
test_index <- sample(1:nrow(dataset), 1000, replace = FALSE)
test_data <- dataset[test_index,]
training_data <- dataset[-test_index,]
train.Y = test_data$purchase_binary_rescale[-test_index]
test.Y = test_data$purchase_binary_rescale[test_index]
train.X = standardized.X[-test_index]
test.X = standardized.X[test_index]
knn(train.X, test.X, train.Y, 1)
train.Y = test_data$Purchase[-test_index]
test.Y = test_data$Purchase[test_index]
train.X = standardized.X[-test_index]
test.X = standardized.X[test_index]
knn(train.X, test.X, train.Y, 1)
len*train.X
len(train.X)
length(train.X)
length(train.Y)
length(train.X)
test_index <- sample(1:nrow(dataset), 1000, replace = FALSE)
test_data <- dataset[test_index,]
training_data <- dataset[-test_index,]
train.Y = test_data$purchase_binary_rescale[-test_index]
test.Y = test_data$purchase_binary_rescale[test_index]
train.X = standardized.X[-test_index]
test.X = standardized.X[test_index]
knn(train.X, test.X, train.Y, 1)
train.X = standardized.X[-test_index,]
test.X = standardized.X[test_index,]
knn(train.X, test.X, train.Y, 1)
knn(training_data, test.X, train.Y, 1)
standardized.X <- data.frame(scale(dataset[,-86]))
# Random 1000 index for test data
test_index <- sample(1:nrow(dataset), 1000, replace = FALSE)
test_data <- dataset[test_index,]
training_data <- dataset[-test_index,]
train.Y = test_data$purchase_binary_rescale[-test_index]
test.Y = test_data$purchase_binary_rescale[test_index]
train.X = standardized.X[-test_index,]
test.X = standardized.X[test_index,]
knn(train.X, test.X, train.Y, 1)
length(train.Y)
length(train.X)
dataset <- Caravan
prop.table(table(dataset$Purchase))
dataset$purchase_binary[dataset$Purchase == 'No'] <- 0
dataset$purchase_binary[dataset$Purchase == 'Yes'] <- 1
dataset$purchase_binary_rescale <- scale(dataset$purchase_binary)
standardized.X <- data.frame(scale(dataset[,-86]))
# Random 1000 index for test data
test_index <- sample(1:nrow(dataset), 1000, replace = FALSE)
test_data <- dataset[test_index,]
training_data <- dataset[-test_index,]
train.Y = test_data$purchase_binary_rescale[-test_index]
test.Y = test_data$purchase_binary_rescale[test_index]
train.X = standardized.X[-test_index,]
test.X = standardized.X[test_index,]
knn(train.X, test.X, train.Y, 1)
train.Y = train_data$purchase_binary_rescale[-test_index]
test.Y = test_data$purchase_binary_rescale[test_index]
train.X = standardized.X[-test_index,]
test.X = standardized.X[test_index,]
knn(train.X, test.X, train.Y, 1)
train.Y = training_data$purchase_binary_rescale[-test_index]
test.Y = test_data$purchase_binary_rescale[test_index]
train.X = standardized.X[-test_index,]
test.X = standardized.X[test_index,]
knn(train.X, test.X, train.Y, 1)
train.Y = training_data$purchase_binary_rescale
test.Y = test_data$purchase_binary_rescale
train.X = standardized.X[-test_index,]
test.X = standardized.X[test_index,]
knn(train.X, test.X, train.Y, 1)
train.Y = dataset$purchase_binary_rescale[-test_index,]
test.Y = dataset$purchase_binary_rescale[test_index,]
train.X = standardized.X[-test_index,]
test.X = standardized.X[test_index,]
knn(train.X, test.X, train.Y, 1)
dataset <- Caravan
prop.table(table(dataset$Purchase))
dataset$purchase_binary[dataset$Purchase == 'No'] <- 0
dataset$purchase_binary[dataset$Purchase == 'Yes'] <- 1
dataset$purchase_binary_rescale <- scale(dataset$purchase_binary)
dataset$standardized.X <- scale(dataset[,-86])
# Random 1000 index for test data
test_index <- sample(1:nrow(dataset), 1000, replace = FALSE)
test_data <- dataset[test_index,]
training_data <- dataset[-test_index,]
train.Y = dataset$purchase_binary_rescale[-test_index,]
test.Y = dataset$purchase_binary_rescale[test_index,]
train.X = dataset$standardized.X[-test_index,]
test.X = dataset$standardized.X[test_index,]
knn(train.X, test.X, train.Y, 1)
mean(test.Y != test.pred)
test.pred <- knn.pred
mean(test.Y != test.pred)
knn.pred <- knn(train.X, test.X, train.Y, k = 1)
mean(test.Y != knn.pred)
table(knnlpred, test.Y)
table(knn.pred, test.Y)
train.Y = dataset$Purchase[-test_index,]
test.Y = dataset$Purchase[test_index,]
train.X = dataset$standardized.X[-test_index,]
test.X = dataset$standardized.X[test_index,]
knn.pred <- knn(train.X, test.X, train.Y, k = 1)
mean(test.Y != knn.pred)
mean(test.Y != "No")
table(knn.pred, test.Y)
dataset <- Caravan
prop.table(table(dataset$Purchase))
dataset$purchase_binary[dataset$Purchase == 'No'] <- 0
dataset$purchase_binary[dataset$Purchase == 'Yes'] <- 1
dataset$purchase_binary_rescale <- scale(dataset$purchase_binary)
dataset$standardized.X <- scale(dataset[,-86])
# Random 1000 index for test data
test_index <- sample(1:nrow(dataset), 1000, replace = FALSE)
# test_data <- dataset[test_index,]
# training_data <- dataset[-test_index,]
train.Y <- dataset$Purchase[-test_index,]
train.Y <- dataset$Purchase[-test_index]
test.Y <- dataset$Purchase[test_index]
train.X <- dataset$standardized.X[-test_index,]
test.X <- dataset$standardized.X[test_index,]
knn.pred <- knn(train.X, test.X, train.Y, k = 1)
mean(test.Y != knn.pred)
mean(test.Y != "No")
table(knn.pred, test.Y)
knn.pred5 <- knn(train.X, test.X, train.Y, k = 5)
table(knn.pred5, test.Y)
knn.pred3 <- knn(train.X, test.X, train.Y, k = 3)
table(knn.pred3, test.Y)
library(chemometrics)
library(chemometrics)
library(class)
library(chemometrics)
library(class)
dataset <- iris
head(iris)
group <- dataset[,5]
X = scale(dataset[, 1:4])
n = nrow(dataset)
# Use 1/3 of dataset as test data
ntrain = round(n * 2/3)
set.seed(123)
train.sample(1:n, ntrain)
train=sample(1:n, ntrain)
resknn = knnEval(X, group, train_index, knnvec = seq(1, 30, by = 1),
legpos = "bottomright")
train_index = sample(1:n, ntrain)
resknn = knnEval(X, group, train_index, knnvec = seq(1, 30, by = 1),
legpos = "bottomright")
title("kNN Classification")
help(legend)
legend(cex = 0.2)
resknn = knnEval(X, group, train_index, knnvec = seq(1, 30, by = 1),
legpos = "bottomright")
title("kNN Classification")
legend(0)
library(ISLR)
library(ISLR)
dataset <- Hitters
library(leaps)
library(leaps)
regfit.full <- regsubsets(dataset$Salary ~ ., dataset, nvmax = 19)
summary(regfit.full)
help(Hitters)
regfit.forward <- regsubsets(dataset$Salary ~ ., dataset, nvmax = 19,
method = "forward")
summary(regfit.forward)
reg.summary <- summary(regfit.full)
# R-squared for each model
reg.summary$rsq
plot(reg.summary$rss, xlab = "# Variables", ylab = "RSS", type = "l")
plot(reg.summary$adjr2, xlab = "# Variables", ylab = "Adjusted R-Squared", type = "l")
which.max(reg.summary$adjr2)
plot(reg.summary$cp, xlab = "# Variables", ylab = "Adjusted R-Squared", type = "l")
which.max(reg.summary$cp)
plot(reg.summary$cp, xlab = "# Variables", ylab = "Cp", type = "l")
which.max(reg.summary$cp)
plot(regfit.full, scale = "bic")
plot(regfit.full, scale = "adjr2")
coef(regfit.full, 5)
dataset <- USArrests
apply(dataset, 2, mean)
pca.out = prcomp(dataset, scale = TRUE)
pca.out
biplot(pca.out, scale = 0)
help(biplot)
biplot(pca.out, scale = 0, cex = 0.75)
biplot(pca.out, scale = 0, cex = 0.5)
race_change_subset <- subset(dataset, incident_to_arrest_days | report_to_arrest_days > 0)
prop.table(table(race_change_subset$black_not_white, race_change_subset$arrestee_race), 1)
options(scipen=999)
prop.table(table(race_change_subset$black_not_white, race_change_subset$arrestee_race), 1)
race_change_subset <- subset(race_change_subset, hispanic != 0)
race_change_subset <- subset(race_change_subset, arrestee_hispanicity != 0)
names(dataset)
race_change_subset <- subset(race_change_subset, arrestee_ethnicity != 'hispanic')
prop.table(table(race_change_subset$black_not_white, race_change_subset$arrestee_race), 1)
describe(dataset$age)
mea(dataset$offender_age)
min(dataset$offender_age)
mean(dataset$offender_age)
mean(dataset$offender_age, rm.na = T)
mean(dataset$offender_age, na.rm = T)
names(dataset)
mean(dataset$age)
mean(dataset$age, na.rm = T)
min(dataset$age)
min(dataset$age, na.rm = T)
table(dataset$region)
table(dataset$state)
names(dataset)
table(dataset$county_no)
table(dataset$region)
table(dataset$county_no)
table(dataset$region)
prop.table(table(dataset$region))
load("/Users/katharina/Dropbox/Mugshots Project/Public/Code/analysis/basic.RData")
# Center continuous IVs around their means
continuous_ivs <- c("w_officers_percent", "b_officers_percent", "male_officers_percent",
"age", "bdgt_ttl", "ftsworn",
"wofficers_divres", "bdgt_per_ftoff",
"agency_offenders_count", "agency_bdgt_per_offender", "agency_bdgt_per_arrestee",
"b_residents_percent", "w_residents_percent", "mean_inc",
"b_unemp", "w_unemp", "one_race_unemp",
"zipcode_bdgt", "zipcode_bdgt_per_resident",
"zipcode_offenders_count", "zipcode_offenders_per_resident", "zipcode_arrestees_per_resident")
# subtract mean, divide by twice the standard deviation
for (iv in continuous_ivs) {
iv_standardized = paste(iv,"standardized", sep = "_")
iv_mean <- mean(simple_assault_data[,iv], na.rm = TRUE)
iv_sd <- sd(simple_assault_data[,iv], na.rm = TRUE)
simple_assault_data[,iv_standardized] <- (simple_assault_data[,iv] - iv_mean) / (2 * iv_sd)
}
simple_assault_data <- subset(dataset, offense_simple_assault == 1)
simple_assault_data <- subset(simple_assault_data, arrested == 0 | arrest_simple_assault == 1)
simple_assault_data_new <- simple_assault_data
# Center continuous IVs around their means
continuous_ivs <- c("w_officers_percent", "b_officers_percent", "male_officers_percent",
"age", "bdgt_ttl", "ftsworn",
"wofficers_divres", "bdgt_per_ftoff",
"agency_offenders_count", "agency_bdgt_per_offender", "agency_bdgt_per_arrestee",
"b_residents_percent", "w_residents_percent", "mean_inc",
"b_unemp", "w_unemp", "one_race_unemp",
"zipcode_bdgt", "zipcode_bdgt_per_resident",
"zipcode_offenders_count", "zipcode_offenders_per_resident", "zipcode_arrestees_per_resident")
# subtract mean, divide by twice the standard deviation
for (iv in continuous_ivs) {
iv_standardized = paste(iv,"standardized", sep = "_")
iv_mean <- mean(simple_assault_data[,iv], na.rm = TRUE)
iv_sd <- sd(simple_assault_data[,iv], na.rm = TRUE)
simple_assault_data_new[,iv_standardized] <- (simple_assault_data[,iv] - iv_mean) / (2 * iv_sd)
}
mean(simple_assault_data_new$w_officers_percent_standardized)
mean(simple_assault_data$w_officers_percent_standardized)
sd(simple_assault_data$w_officers_percent_standardized)
sd(simple_assault_data_new$w_officers_percent_standardized)
hist(simple_assault_data$w_officers_percent_standardized)
hist(simple_assault_data_new$w_officers_percent_standardized)
lapply(intercept_models_wvictims, function(model) {
AIC(model)
})
lapply(intercept_models, function(model) {
AIC(model)
})
lapply(coef_models_wvictims, function(model) {
AIC(model)
})
lapply(coef_models, function(model) {
AIC(model)
})
lapply(models_list, function(model) {
model@optinfo$conv$lme4$messages
})
models_list[[9]]
summary(models_list[[9]])
lapply(models_list_wvictims, function(model) {
model@optinfo$conv$lme4$messages
})
cite("R")
cite(R)
citation(R)
citation("R")
citation()
rm(list = ls())
options(scipen=999)
# Read the merged and twice-cleaned data from saved session
load("~/Dropbox/Projects/Mugshots Project/Data/basic.RData")
table(dataset$race)
rm(list = ls())
options(scipen = 999)
# Read the cleaned data from saved session
load("~/Dropbox/Projects/Mugshots Project/Data/basic.RData")
prop.table(table(dataset$black_not_white))
prop.table(table(subset(dataset, arrested == 1)$black_not_white))
rm(list = ls())
options(scipen=999)
# Read the cleaned data from saved session
load("~/Dropbox/Projects/Mugshots Project/Data/basic.RData")
offenses <- c("robbery", "aggravated_assault", "simple_assault", "intimidation", "weapon",
"shoplifting", "vandalism",  "drugs_narcotics", "drug_equipment")
offenders <- paste0("offense_", offenses)
arrestees <- paste0("arrested_", offenses)
offenders
dataset$offender_in_eight <- 0
for (offender in offenders) {
dataset[dataset[, offender] == 1]$offender_in_eight <- 1
}
dataset$offender_in_eight <- 0
for (offender in offenders) {
print(offender)
dataset[dataset[, offender] == 1]$offender_in_eight <- 1
}
dataset$offender_in_eight <- 0
offense
offense = "offense_robbery"
dataset[dataset[, offender] == 1]$offender_in_eight <- 1
dataset$offender_in_eight[dataset[, offender] == 1] <- 1
dataset$offender_in_eight <- 0
for (offender in offenders) {
print(offender)
dataset$offender_in_eight[dataset[, offender] == 1] <- 1
}
dataset$arrestee_in_eight <- 0
for (arrestee in arrestees) {
dataset$arrestee_in_eight[dataset[, arrestee] == 1] <- 1
}
dataset$arrestee_in_eight <- 0
for (arrestee in arrestees) {
print(arrestee)
dataset$arrestee_in_eight[dataset[, arrestee] == 1] <- 1
}
arrestees <- paste0("arrest_", offenses)
dataset$arrestee_in_eight <- 0
for (arrestee in arrestees) {
print(arrestee)
dataset$arrestee_in_eight[dataset[, arrestee] == 1] <- 1
}
table(dataset$offender_in_eight)
table(dataset$arrestee_in_eight)
prop.table(dataset$offender_in_eight)
table(prop.table(dataset$offender_in_eight))
prop.table(table(dataset$offender_in_eight))
prop.table(table(dataset$arrestee_in_eight))
dataset$arrestee_in_eight <- 0
dataset$arrestee_in_eight[dataset$arrested == 0] <- NA
for (arrestee in arrestees) {
dataset$arrestee_in_eight[dataset[, arrestee] == 1] <- 1
}
prop.table(table(dataset$arrestee_in_eight))
table(dataset$black_not_white)
